{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install llama-index #LLM inteface LLM with your private Data \n",
    "#!pip install langchain #application building with LLM\n",
    "# !pip install jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install ipywidgets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download  your custom Dataset\n",
    "We are going to use github as our knowledge base. You can upload your own custom dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'chatbottest'...\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/danishmeh/chatbottest/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the functions\n",
    "The following code defines the functions we need to construct the index and query it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader, GPTListIndex, readers, GPTSimpleVectorIndex, LLMPredictor, PromptHelper, ServiceContext\n",
    "from langchain import OpenAI\n",
    "import sys\n",
    "import os\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def construct_index(directory_path): #directory patch Context_data --> Data \n",
    "    # set maximum input size\n",
    "    max_input_size = 4096\n",
    "    # set number of output tokens\n",
    "    num_outputs = 2000\n",
    "    # set maximum chunk overlap\n",
    "    max_chunk_overlap = 20\n",
    "    # set chunk size limit\n",
    "    chunk_size_limit = 600 \n",
    "\n",
    "    # define LLM\n",
    "    llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.5, model_name=\"text-davinci-003\", max_tokens=num_outputs))\n",
    "    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)\n",
    " \n",
    "    documents = SimpleDirectoryReader(directory_path).load_data()\n",
    "    \n",
    "    #updated April 2023 \n",
    "    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
    "\n",
    "    index = GPTSimpleVectorIndex.from_documents(\n",
    "    documents, service_context=service_context\n",
    "    )\n",
    "\n",
    "    index.save_to_disk('index.json') #here we are loading the data and saving the index in json Vactor map of Token  \n",
    "\n",
    "    return index\n",
    "\n",
    "def ask_bot():\n",
    "    index = GPTSimpleVectorIndex.load_from_disk('index.json')\n",
    "    while True: \n",
    "        query = input(\"Ask you question ? \")\n",
    "        response = index.query(query, response_mode=\"compact\")\n",
    "        display(Markdown(f\"Response: <b>{response.response}</b>\"))\n",
    "  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set OpenAI API Key\n",
    "You need an OPENAI API key to be able to run this code.\n",
    "\n",
    "[signing up](https://platform.openai.com/overview). \n",
    "\n",
    "Then run the code below and paste your API key into the text input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = input(\"Paste OpenAI API key here and enter:\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct an index\n",
    "Now we are ready to construct the index. This will take every file in the folder 'data', split it into chunks, and embed it with OpenAI's embeddings API.\n",
    "\n",
    "**Notice:** running this code will cost you credits on your OpenAPI account ($0.02 for every 1,000 tokens). If you've just set up your account, the free credits that you have should be more than enough for this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 706 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<llama_index.indices.vector_store.vector_indices.GPTSimpleVectorIndex at 0x247ea8ed190>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "construct_index(\"chatbottest/Data\") #tokanising "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ask questions\n",
    "It's time to have fun and test our AI. Run the function that queries GPT and type your question into the input. \n",
    "\n",
    "If you've used the provided example data for your custom knowledge base, here are a few questions that you can ask:\n",
    "1. How to connect to yout techbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 748 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 3 tokens\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Response: <b>\n",
       "Kzz01 is a coal mill fan actuator.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 799 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 5 tokens\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Response: <b>and outputs of kzz01?\n",
       "\n",
       "The inputs of kzz01 are setpoint (kspzz01) and feedback (kfpzz01). The outputs of kzz01 are 3phase breaker K01, setpoint (kspzz01) and feedback (kfpzz01).</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 785 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 7 tokens\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Response: <b>\n",
       "The purpose of an actuator is to convert energy into motion in order to control a mechanical system. It is typically used to move or control a mechanism or system, such as opening a valve or moving a robot arm.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 775 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 4 tokens\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Response: <b>\n",
       "When the actuator is operated, it will move the fan blades to adjust the air flow in the coal mill. This will help to regulate the temperature and pressure in the coal mill.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ask_bot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`**Stop words(NLP)**`\n",
    "are common words that are often removed from texts before processing, \n",
    "as they are unlikely to carry much meaning or importance in natural language\n",
    "processing tasks. \n",
    "`Here are some examples of common stop words in the English language`\n",
    "a, an, and, as, at, be, but, by, for, if, in, into, is, it, no, not, of, on, or,\n",
    "such, that, the, their, then, there, these, they, this, to, was, will, with.\n",
    "`why Remove stop words in NLP`\n",
    "Certainly! Removing stop words from text helps to reduce noise in data, improve text analysis efficiency, and increase accuracy of natural language processing tasks like sentiment analysis or topic modeling. This is because stop words don't carry much meaning or significance in a given context and can often be misleading or ambiguous. By removing stop words, we focus on the more meaningful words in a text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
